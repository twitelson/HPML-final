{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4z7vmI9ziLJh"
      },
      "outputs": [],
      "source": [
        "# This notebook is based on PyTorch's Knowledge Distillation Tutorial, found here:\n",
        "# https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilVqjbmta3ah",
        "outputId": "e4cd5a56-157a-4f73-82ad-6463da13a5fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to W&B account\n",
        "import wandb"
      ],
      "metadata": {
        "id": "6UK7WS2ga4Af"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "3uNF0cB0a8VM",
        "outputId": "f536c866-8dad-47d3-aae6-04a1eeb6d8a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uvcCwqLHiLJm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Check if GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "4URtymqXM7Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "wt4zy4ulNJ7I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv(\"initial_train_dataframe.csv\")\n",
        "y = pd.read_csv(\"initial_label_dataframe.csv\")"
      ],
      "metadata": {
        "id": "N_yH1mLuNFZU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [ \"arrival_0\", \"arrival_15\", \"arrival_30\", \"arrival_45\", \"depart_0\", \"depart_15\", \"depart_30\", \"depart_45\", \"quarter_0\", \"quarter_15\", \"quarter_30\", \"quarter_45\", \"month_0\", \"day_of_week\"]\n",
        "labels= [\"arrival_0_1H\", \"arrival_15_1H\", \"arrival_30_1H\", \"arrival_45_1H\", \"arrival_0_2H\",  \"arrival_15_2H\", \"arrival_30_2H\", \"arrival_45_2H\", \"arrival_0_3H\", \"arrival_15_3H\", \"arrival_30_3H\", \"arrival_45_3H\"]\n",
        "indices = list(range(0, X.shape[0]))\n",
        "trainIndex = int(0.8*len(indices))\n",
        "trainTensor = torch.tensor((X[numeric_features]).values[indices[0:trainIndex]], device=\"cpu\").float()\n",
        "trainLabels = torch.tensor((y[labels]).values[indices[0:trainIndex]], device=\"cpu\").float()\n",
        "valTensor = torch.tensor((X[numeric_features]).values[indices[trainIndex:-14]], device=\"cpu\").float()\n",
        "valLabels = torch.tensor((y[labels]).values[indices[trainIndex:-14]], device=\"cpu\").float()\n",
        "\n",
        "train_dataset = TensorDataset(trainTensor, trainLabels)\n",
        "test_dataset = TensorDataset(valTensor, valLabels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=56, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=56, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "At_lAMVcM5i_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Va5adXiLJo"
      },
      "source": [
        "Defining model classes and utility functions\n",
        "============================================\n",
        "\n",
        "Next, we need to define our model classes. Several user-defined\n",
        "parameters need to be set here. We use two different architectures,\n",
        "keeping the number of filters fixed across our experiments to ensure\n",
        "fair comparisons. Both architectures are Convolutional Neural Networks\n",
        "(CNNs) with a different number of convolutional layers that serve as\n",
        "feature extractors, followed by a classifier with 12 classes. The number\n",
        "of filters and neurons is smaller for the students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W7t08k1EiLJp"
      },
      "outputs": [],
      "source": [
        "# Deeper neural network class to be used as teacher:\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(14, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(64, 128, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(64, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Lightweight neural network class to be used as student:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(14, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yqZ_ohb2iLJp"
      },
      "outputs": [],
      "source": [
        "loss_fn = lambda x,y: torch.exp(-torch.sqrt(((x-y)**2).mean())/10)\n",
        "\n",
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = -loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # log loss using W and B\n",
        "        wandb.log({\"loss\": running_loss / len(train_loader)})\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "\n",
        "    start = time.time() # record inference time\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    squared_error = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            total += 1\n",
        "            squared_error += loss_fn(outputs, labels)\n",
        "\n",
        "    accuracy = squared_error / total\n",
        "    print(f\"Test acc: {accuracy:.4f}\")\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Elapsed time: {end-start:.2f}\")\n",
        "    wandb.log({\"time\": end-start}) # log inference time\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0CmlLPOiLJq"
      },
      "source": [
        "Initial training runs\n",
        "==================\n",
        "\n",
        "For reproducibility, we set the torch manual seed. We train\n",
        "networks using different methods, so to compare them fairly, it makes\n",
        "sense to initialize the networks with the same weights. Start by\n",
        "training the teacher network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "doFolhDciLJq",
        "outputId": "7d1fc9f2-24ee-404e-faa2-505d6ea67199"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_211750-1fnlkfov</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1fnlkfov' target=\"_blank\">deep_nn_train</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1fnlkfov' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1fnlkfov</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: -0.4648365443126081\n",
            "Epoch 2/15, Loss: -0.5102230784611199\n",
            "Epoch 3/15, Loss: -0.5132005197552446\n",
            "Epoch 4/15, Loss: -0.5132960943749156\n",
            "Epoch 5/15, Loss: -0.513472133646377\n",
            "Epoch 6/15, Loss: -0.5132478924985892\n",
            "Epoch 7/15, Loss: -0.5131508321426927\n",
            "Epoch 8/15, Loss: -0.5136609368811781\n",
            "Epoch 9/15, Loss: -0.513717073030746\n",
            "Epoch 10/15, Loss: -0.514194377980674\n",
            "Epoch 11/15, Loss: -0.5146414720402739\n",
            "Epoch 12/15, Loss: -0.5143257230044173\n",
            "Epoch 13/15, Loss: -0.514994304115399\n",
            "Epoch 14/15, Loss: -0.5152962920003044\n",
            "Epoch 15/15, Loss: -0.5155271582138805\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-0.51553</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep_nn_train</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1fnlkfov' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1fnlkfov</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_211750-1fnlkfov/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=12).to(device)\n",
        "wandb.init(\n",
        "    # Set the project\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"deep_nn_train\",\n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    \"epochs\": 15,\n",
        "    })\n",
        "train(nn_deep, train_loader, epochs=15, learning_rate=0.001, device=device)\n",
        "\n",
        "# Mark the run as finished\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"deep_nn_test\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "oC0Z9NcYkeLa",
        "outputId": "bfb4d151-67c0-4617-c481-16fa26e9e125"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_211828-n1l76fgw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n1l76fgw' target=\"_blank\">deep_nn_test</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n1l76fgw' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n1l76fgw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6357\n",
            "Elapsed time: 0.34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep_nn_test</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n1l76fgw' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n1l76fgw</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_211828-n1l76fgw/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the lightweight network:\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=12).to(device)"
      ],
      "metadata": {
        "id": "9I7OO7IEknRc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIgGkbkNiLJq"
      },
      "source": [
        "We instantiate one more lightweight network model to compare their\n",
        "performances. Back propagation is sensitive to weight initialization, so\n",
        "we need to make sure these two networks have the exact same\n",
        "initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sxaE-MWQiLJq"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=12).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz4GhmxkiLJs"
      },
      "source": [
        "To ensure we have created a copy of the first network, we inspect the\n",
        "norm of its first layer. If it matches, then we are safe to conclude\n",
        "that the networks are indeed the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eDxl-MziLJt",
        "outputId": "9f850812-660b-462b-fd4f-bdab0e5449e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm of 1st layer of nn_light: 2.190237045288086\n",
            "Norm of 1st layer of new_nn_light: 2.190237045288086\n"
          ]
        }
      ],
      "source": [
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMhFeY5XiLJt"
      },
      "source": [
        "Print the total number of parameters in each model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPkQDrFAiLJt",
        "outputId": "f295a71a-7cb5-40b9-e630-353119f2ecb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepNN parameters: 22,938\n",
            "LightNN parameters: 2,602\n"
          ]
        }
      ],
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsdCrEpbiLJt"
      },
      "source": [
        "Train and test the lightweight network with cross entropy loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "d22iL_yeiLJu",
        "outputId": "cc5c9646-46f1-46de-ffcb-06bfb4af2c2b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_211846-0d3swrx2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/0d3swrx2' target=\"_blank\">light_nn_train</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/0d3swrx2' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/0d3swrx2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: -0.4673674910213239\n",
            "Epoch 2/15, Loss: -0.5052133432973307\n",
            "Epoch 3/15, Loss: -0.5111270848744974\n",
            "Epoch 4/15, Loss: -0.5123393821259277\n",
            "Epoch 5/15, Loss: -0.5134803881279577\n",
            "Epoch 6/15, Loss: -0.5141326758427361\n",
            "Epoch 7/15, Loss: -0.5152957408001628\n",
            "Epoch 8/15, Loss: -0.5158408610775067\n",
            "Epoch 9/15, Loss: -0.5168480135190981\n",
            "Epoch 10/15, Loss: -0.5182226090766371\n",
            "Epoch 11/15, Loss: -0.5189879449030843\n",
            "Epoch 12/15, Loss: -0.5189019357815338\n",
            "Epoch 13/15, Loss: -0.5200413916819393\n",
            "Epoch 14/15, Loss: -0.5204711543104519\n",
            "Epoch 15/15, Loss: -0.5201884551931875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-0.52019</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light_nn_train</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/0d3swrx2' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/0d3swrx2</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_211846-0d3swrx2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"light_nn_train\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    \"epochs\": 15,\n",
        "    })\n",
        "train(nn_light, train_loader, epochs=15, learning_rate=0.001, device=device)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-d1nsD6iLJu"
      },
      "source": [
        "As we can see, based on test accuracy, we can now compare the deeper\n",
        "network that is to be used as a teacher with the lightweight network\n",
        "that is our supposed student. So far, our student has not intervened\n",
        "with the teacher, therefore this performance is achieved by the student\n",
        "itself. The metrics so far can be seen with the following lines:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"light_nn_test\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_light = test(nn_light, test_loader, device)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "YiY8E8nsflXC",
        "outputId": "9d95393b-d3f5-4cce-cc22-169831ff31a8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_211937-1kpuc2nb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1kpuc2nb' target=\"_blank\">light_nn_test</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1kpuc2nb' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1kpuc2nb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6372\n",
            "Elapsed time: 0.44\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light_nn_test</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1kpuc2nb' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/1kpuc2nb</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_211937-1kpuc2nb/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8UJxfW8iLJu",
        "outputId": "4865f5a4-382a-4ec7-a910-b7bcdedca133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher accuracy: 0.6357%\n",
            "Student accuracy: 0.6372%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.4f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_light:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge distillation run: response-based knowledge\n",
        "===================================================="
      ],
      "metadata": {
        "id": "Hw9zmxYJY237"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7_GrwCaKiLJu",
        "outputId": "ee146037-e71c-4361-9eb3-399fd2c80a74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_212117-n6fo5ph5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n6fo5ph5' target=\"_blank\">light_nn_train_KD</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n6fo5ph5' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n6fo5ph5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: -0.5356747969842186\n",
            "Epoch 2/15, Loss: -0.6031586674455637\n",
            "Epoch 3/15, Loss: -0.6107667301790402\n",
            "Epoch 4/15, Loss: -0.6130794317196733\n",
            "Epoch 5/15, Loss: -0.6133716441571903\n",
            "Epoch 6/15, Loss: -0.6141026618000799\n",
            "Epoch 7/15, Loss: -0.6156381351498369\n",
            "Epoch 8/15, Loss: -0.6158110628874538\n",
            "Epoch 9/15, Loss: -0.6163829123250212\n",
            "Epoch 10/15, Loss: -0.6178820851130988\n",
            "Epoch 11/15, Loss: -0.6193491559439954\n",
            "Epoch 12/15, Loss: -0.6202494393522366\n",
            "Epoch 13/15, Loss: -0.6215826855680813\n",
            "Epoch 14/15, Loss: -0.6224061338284526\n",
            "Epoch 15/15, Loss: -0.6224503560949819\n",
            "Teacher accuracy: 0.6357%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-0.62245</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light_nn_train_KD</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n6fo5ph5' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/n6fo5ph5</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_212117-n6fo5ph5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_212151-23hxtzxf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/23hxtzxf' target=\"_blank\">light_nn_test_2</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/23hxtzxf' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/23hxtzxf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6378\n",
            "Elapsed time: 0.32\n",
            "Student accuracy without teacher: 0.6378%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light_nn_test_2</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/23hxtzxf' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/23hxtzxf</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_212151-23hxtzxf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_212156-rtbg7lzd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/rtbg7lzd' target=\"_blank\">light_nn_test_KD</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/rtbg7lzd' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/rtbg7lzd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6399\n",
            "Elapsed time: 0.32\n",
            "Student accuracy with KD: 0.6399%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light_nn_test_KD</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/rtbg7lzd' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/rtbg7lzd</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_212156-rtbg7lzd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher accuracy: 0.6357%\n",
            "Student accuracy without teacher: 0.6378%\n",
            "Student accuracy with CE + KD: 0.6399%\n"
          ]
        }
      ],
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, target_loss_weight, loss_weight, device):\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_out = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_out = student(inputs)\n",
        "\n",
        "            # student-teacher loss\n",
        "            student_teacher_loss = loss_fn(student_out, teacher_out)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = loss_fn(student_out, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = -1 * (target_loss_weight * student_teacher_loss + loss_weight * label_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        wandb.log({\"loss\": running_loss / len(train_loader)})\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Apply ``train_knowledge_distillation``\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"light_nn_train_KD\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    \"epochs\": 15,\n",
        "    })\n",
        "\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=15, learning_rate=0.001, target_loss_weight=0.25, loss_weight=0.75, device=device)\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.4f}%\")\n",
        "wandb.finish()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"light_nn_test_2\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_light = test(nn_light, test_loader, device)\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light:.4f}%\")\n",
        "wandb.finish()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"light_nn_test_KD\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_light_kd = test(new_nn_light, test_loader, device)\n",
        "print(f\"Student accuracy with KD: {test_accuracy_light_kd:.4f}%\")\n",
        "wandb.finish()\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.4f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light:.4f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_kd:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTIvG2XiLJu"
      },
      "source": [
        "Cosine loss minimization run: KD with feature-based knowledge\n",
        "============================\n",
        "\n",
        "Now we add a new loss function to convey the information in the teacher's hidden layers. Our goal is to minimize this naive cosine loss function, thereby\n",
        "making the student's flattened hidden layer more similar to that of the teacher.\n",
        "\n",
        "We also include an average pooling layer to ensure that the vectors are the same\n",
        "size so the cosine loss function is well defined.\n",
        "\n",
        "The `CosineEmbeddingLoss`\n",
        "is given by the following formula:\n",
        "\n",
        "![Formula for\n",
        "CosineEmbeddingLoss](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_embedding_loss.png){.align-center\n",
        "width=\"450px\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lpSJSJJiLJv",
        "outputId": "76704517-39f9-4e10-f8fb-2fa6da5a5a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm of 1st layer for deep_nn: 2.5286672115325928\n",
            "Norm of 1st layer for modified_deep_nn: 2.5286672115325928\n",
            "Norm of 1st layer: 2.190237045288086\n"
          ]
        }
      ],
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(14, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(64, 128, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(64, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 1)\n",
        "        return x, flattened_conv_output_after_pooling\n",
        "\n",
        "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(14, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        return x, flattened_conv_output\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=12).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Once again ensure the norm of the first layer is the same for both networks\n",
        "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
        "\n",
        "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=12).to(device)\n",
        "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF_Ds9fdiLJv"
      },
      "source": [
        "Next, we modify the training loop to account for the tuple that the forward pass returns which includes the output and the hidden representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for inputs, labels in train_loader:\n",
        "#   print(inputs.shape, labels.shape)\n",
        "#   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sIThlp8ykwh",
        "outputId": "d96b973e-b2f5-4662-9a26-9db4e829c829"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([56, 14]) torch.Size([56, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i50g4g6-iLJv",
        "outputId": "9925471c-142e-485f-d321-02c6e47492f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student logits shape: torch.Size([56, 12])\n",
            "Student hidden representation shape: torch.Size([56, 4])\n",
            "Teacher logits shape: torch.Size([56, 12])\n",
            "Teacher hidden representation shape: torch.Size([56, 4])\n"
          ]
        }
      ],
      "source": [
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(56, 14).to(device)\n",
        "\n",
        "# Pass the input through the student\n",
        "light_outputs, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Student logits shape:\", light_outputs.shape) # batch_size x total_classes\n",
        "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# Pass the input through the teacher\n",
        "light_outputs, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Teacher logits shape:\", light_outputs.shape) # batch_size x total_classes\n",
        "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PxgZ2e_riLJv"
      },
      "outputs": [],
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, loss_weight, device):\n",
        "\n",
        "    cosine_loss = nn.CosineEmbeddingLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model and keep only the hidden representation\n",
        "            with torch.no_grad():\n",
        "                _, teacher_hidden_representation = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_outs, student_hidden_representation = student(inputs)\n",
        "\n",
        "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
        "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = -loss_fn(student_outs, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = hidden_rep_loss_weight * hidden_rep_loss + loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        wandb.log({\"loss\": running_loss / len(train_loader)})\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlK41kv6iLJw"
      },
      "source": [
        "We need to modify our test function for the same reason. Here we ignore\n",
        "the hidden representation returned by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KBkLjKdfiLJw"
      },
      "outputs": [],
      "source": [
        "def test_multiple_outputs(model, test_loader, device):\n",
        "\n",
        "    start = time.time() # record inference time\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    squared_error = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs)\n",
        "\n",
        "            total += 1\n",
        "            squared_error += loss_fn(outputs, labels)\n",
        "\n",
        "    accuracy = squared_error / total\n",
        "    print(f\"Test acc: {accuracy:.4f}\")\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Elapsed time: {end-start:.4f}\")\n",
        "    wandb.log({\"time\": end-start}) # log inference time\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aOMaFckiLJw"
      },
      "source": [
        "In this case, we could easily include both knowledge distillation and\n",
        "cosine loss minimization in the same function. It is common to combine\n",
        "methods to achieve better performance in teacher-student paradigms. For\n",
        "now, we can run a simple train-test session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "6h7G1brGiLJw",
        "outputId": "843a37de-4645-4f7d-cf7e-d32d032a107b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_212353-wpdyx1r9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/wpdyx1r9' target=\"_blank\">cosine_loss_train</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/wpdyx1r9' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/wpdyx1r9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: -0.20772462216809914\n",
            "Epoch 2/15, Loss: -0.2606479236112235\n",
            "Epoch 3/15, Loss: -0.2638599706914859\n",
            "Epoch 4/15, Loss: -0.2656288054137946\n",
            "Epoch 5/15, Loss: -0.2667099564505842\n",
            "Epoch 6/15, Loss: -0.26694401070332757\n",
            "Epoch 7/15, Loss: -0.26906462556447464\n",
            "Epoch 8/15, Loss: -0.2700655847883072\n",
            "Epoch 9/15, Loss: -0.2712587409030896\n",
            "Epoch 10/15, Loss: -0.2733841678395439\n",
            "Epoch 11/15, Loss: -0.27719020857788124\n",
            "Epoch 12/15, Loss: -0.2829544541363518\n",
            "Epoch 13/15, Loss: -0.28555448205707173\n",
            "Epoch 14/15, Loss: -0.28762641806191147\n",
            "Epoch 15/15, Loss: -0.2879176004626119\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▃▃▃▃▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-0.28792</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cosine_loss_train</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/wpdyx1r9' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/wpdyx1r9</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_212353-wpdyx1r9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_212430-mkfmv6mw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/mkfmv6mw' target=\"_blank\">cosine_loss_test</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/mkfmv6mw' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/mkfmv6mw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6408\n",
            "Elapsed time: 0.2957\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>time</td><td>0.29571</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cosine_loss_test</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/mkfmv6mw' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/mkfmv6mw</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_212430-mkfmv6mw/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train and test the lightweight network\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"cosine_loss_train\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    \"epochs\": 15,\n",
        "    })\n",
        "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=15, learning_rate=0.001, hidden_rep_loss_weight=0.25, loss_weight=0.75, device=device)\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"cosine_loss_test\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_light_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjRV-rTOiLJw"
      },
      "source": [
        "Intermediate regressor run: KD with relation-based knowledge\n",
        "==========================\n",
        "\n",
        "Our naive cosine loss minimization does not guarantee better results, so we will try relation-based knowledge, implemented through an intermediate regressor which minimizes MSE between the feature maps of the student and teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY9gKTH0iLJx",
        "outputId": "fbf92be1-644c-4026-d15b-1cfa12ea8903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student's feature extractor output shape:  torch.Size([56, 4])\n",
            "Teacher's feature extractor output shape:  torch.Size([56, 4])\n"
          ]
        }
      ],
      "source": [
        "# Pass the sample input only from the convolutional feature extractor\n",
        "convolutional_fe_output_student = nn_light.features(sample_input)\n",
        "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
        "\n",
        "# Print their shapes\n",
        "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
        "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iNgi8pOXiLJ1"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(14, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(64, 128, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv1d(64, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        conv_feature_map = x\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(56, 14, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "            nn.Conv1d(14, 56, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=1, stride=2),\n",
        "        )\n",
        "        # Include an extra regressor (in our case linear)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv1d(56, 56, kernel_size=1, padding=0)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4, 56),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(56, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3t1SxghiLJ2"
      },
      "source": [
        "After that, we have to update our train loop again. This time, we\n",
        "extract the regressor output of the student, the feature map of the\n",
        "teacher, we calculate the `MSE` on these tensors (they have the exact\n",
        "same shape so it\\'s properly defined) and we back propagate gradients\n",
        "based on that loss, in addition to our loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "jbkA3oqxiLJ2",
        "outputId": "44764141-a8ef-421c-e379-9f6194a47ec5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_213128-6kwt0alh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/6kwt0alh' target=\"_blank\">relation_KD_train</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/6kwt0alh' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/6kwt0alh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: -0.11737132677064537\n",
            "Epoch 2/15, Loss: -0.2624121413539393\n",
            "Epoch 3/15, Loss: -0.2819834760488413\n",
            "Epoch 4/15, Loss: -0.29507492882565567\n",
            "Epoch 5/15, Loss: -0.31700853812999236\n",
            "Epoch 6/15, Loss: -0.3277685209965934\n",
            "Epoch 7/15, Loss: -0.33239490260331395\n",
            "Epoch 8/15, Loss: -0.3370368047453725\n",
            "Epoch 9/15, Loss: -0.3376909791947173\n",
            "Epoch 10/15, Loss: -0.34245735811539735\n",
            "Epoch 11/15, Loss: -0.34240963331426677\n",
            "Epoch 12/15, Loss: -0.3402695272105951\n",
            "Epoch 13/15, Loss: -0.3390910074162407\n",
            "Epoch 14/15, Loss: -0.3363130316852381\n",
            "Epoch 15/15, Loss: -0.3389919742989464\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-0.33899</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">relation_KD_train</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/6kwt0alh' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/6kwt0alh</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_213128-6kwt0alh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_213203-vshgkq8i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/vshgkq8i' target=\"_blank\">relation_KD_test</a></strong> to <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/vshgkq8i' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/vshgkq8i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.6320\n",
            "Elapsed time: 0.7479\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>time</td><td>0.74786</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">relation_KD_test</strong> at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/vshgkq8i' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD/runs/vshgkq8i</a><br/> View project at: <a href='https://wandb.ai/tw2848-columbia-university/HPML_KD' target=\"_blank\">https://wandb.ai/tw2848-columbia-university/HPML_KD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_213203-vshgkq8i/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, loss_weight, device):\n",
        "\n",
        "    mse_loss = nn.MSELoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Again ignore teacher outputs\n",
        "            with torch.no_grad():\n",
        "                _, teacher_feature_map = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, regressor_feature_map = student(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = -loss_fn(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = feature_map_weight * hidden_rep_loss + loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        wandb.log({\"loss\": running_loss / len(train_loader)})\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "\n",
        "# Initialize a ModifiedLightNNRegressor\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=12).to(device)\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=12).to(device)\n",
        "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Train and test once again\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"relation_KD_train\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    \"epochs\": 15,\n",
        "    })\n",
        "train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=15, learning_rate=0.001, feature_map_weight=0.25, loss_weight=0.75, device=device)\n",
        "wandb.finish()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML_KD\",\n",
        "    name=f\"relation_KD_test\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"NASA Airport Throughput Challenge - FUSER Dataset\",\n",
        "    # \"epochs\": 15,\n",
        "    })\n",
        "test_accuracy_light_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jraFTDwiLJ2",
        "outputId": "5a6ccf17-c49a-4228-91e5-58b00f75b2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher accuracy: 0.6357%\n",
            "Student accuracy without teacher: 0.6378%\n",
            "Student accuracy with KD: 0.6399%\n",
            "Student accuracy with CosineLoss: 0.6408%\n",
            "Student accuracy with RegressorMSE: 0.6320%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.4f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light:.4f}%\")\n",
        "print(f\"Student accuracy with KD: {test_accuracy_light_kd:.4f}%\")\n",
        "print(f\"Student accuracy with CosineLoss: {test_accuracy_light_cosine_loss:.4f}%\")\n",
        "print(f\"Student accuracy with RegressorMSE: {test_accuracy_light_mse_loss:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atu2jAPeLhj4"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}